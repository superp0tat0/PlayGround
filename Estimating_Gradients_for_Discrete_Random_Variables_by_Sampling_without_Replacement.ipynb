{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Estimating Gradients for Discrete Random Variables by Sampling without Replacement.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superp0tat0/PlayGround/blob/master/Estimating_Gradients_for_Discrete_Random_Variables_by_Sampling_without_Replacement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:28.812657Z",
          "start_time": "2021-01-27T21:43:27.728788Z"
        },
        "id": "YpPSizOS03WT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a5a25b44-0d5f-4dcb-97c3-0e306963cb91"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import abstract_experiments_lib as ab_lib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import optimization_lib as optim_lib\n",
        "import baselines_lib as bs_lib\n",
        "from scipy.stats import dirichlet\n",
        "\n",
        "os.makedirs('plots', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae6463de9da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mabstract_experiments_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mab_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'abstract_experiments_lib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf6eLoFc1r_D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "8107919d-d619-4ed2-f6cb-528d7bece53b"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t80ejhHe03WW"
      },
      "source": [
        "# The reimplementation and demonstration of Published Paper:\n",
        "\n",
        "**Estimating Gradients For Discrete Random Variables By Sampling Without Replacement**\n",
        "\n",
        "## Outline\n",
        "\n",
        "The Key idea of this paper is the Rao Blackwellized estimators are better than the current estimators used in Gradient Estimation problems. Authors derived a novel, unbiased gradient estimator for discrete random variables based on sampling without replacement. They relate their estimator to existing multi-sample estimators and motivate why we would expect reduced variance. Finally, they evaluate their estimator across several tasks and show that it performs well in all of them.\n",
        "\n",
        "In this notebook, we demonstrated the key idea of this paper is intuitive and easy to follow using an example related to genotype estimation based on blood type. Traditionally, this example could be solved by the Markov Chain Monte Carlo approach. In this case we choose Gibbs Sampler as our estimator. By comparison, we also derived the Rao Blackwellized version of Gibbs Sampler and showed the variance is significantly smaller and their biases are close.\n",
        "\n",
        "We also reimplemented the algorithms demonstrated in paper and applied them on two different distributions. Such re-implementations verified Rao Blackwellized Estimators are better than most of the benchmark estimators in gradient estimation problems.\n",
        "\n",
        "The authors also argue this estimator could perform well on both high and low entropy circumstances. However, we think such argument is fragile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWpFMf403WW"
      },
      "source": [
        "# Demonstration of the Key idea\n",
        "\n",
        "## Rao Blackwell Theorem\n",
        "If we have a estimator $g(X)$ of parameter $\\theta$, Given a sufficient statistics $T(X)$. We could derive the Rao Blackwellized version of this estimator $g^*(X) = \\mathbf{E}(g(X) | T(X))$ is a better estimator than g(X) itself of $\\theta$, at least not worse due the following properties:\n",
        "\n",
        "\n",
        "1. $Var(g^*(X)) \\leq Var(g(X))$\n",
        "1. If $g(X)$ is unbiased, so as $g^*(X)$.\n",
        "\n",
        "We will use a simple example to demonstrate this idea\n",
        "\n",
        "\n",
        "## Gibbs Sampler VS. Rao Blackwellized Gibbs Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdgc_rAL03WX"
      },
      "source": [
        "Suppose we sample some patients randomly from the hospital. We then collect the blood type of those patients and get the following data.\n",
        "The first table showed the number of patients with the corresponding blood types\n",
        "\n",
        "|  A | B  | AB  | O  |\n",
        "|---|---|---|---|\n",
        "|  724 | 158  | 87  | 787  |\n",
        "\n",
        "The Second table showed the relations between the unobserved Genotype, the observed Blood Type and their probabilities.\n",
        "\n",
        "|  Genotype | Probability  | Observed B-Type  | Probability  |\n",
        "|---|---|---|---|\n",
        "| AA  | $p_A^2$  | A  | $2p_Ap_O + p_A^2$  |\n",
        "|  AO | $2p_Ap_O$  | A  | $2p_Ap_O + p_A^2$  |\n",
        "| BB  | $p_B^2$  | B  | $2p_Bp_O + p_B^2$  |\n",
        "|  BO | $2p_Bp_O$  |  B | $2p_Bp_O + p_B^2$  |\n",
        "| AB  | $2p_Ap_B$  | AB  | $2p_Ap_B$  |\n",
        "| OO  |  $p_O^2$ |  O | $p_O^2$   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd9lluue03WX"
      },
      "source": [
        "Assume $Z_1 = G(AA)$, $Z_2 = G(BB)$, $y_1 = B(AB)$, $y_2 = B(A)$, $y_3 = B(B)$ and $y_4 = B(O)$\n",
        "Then\n",
        "\n",
        "$\\begin{align*}\n",
        "    &P(\\mathbf{Y, Z} | \\mathbb{\\theta}) &\\\\ &= \n",
        "    (p_Ap_B)^{y_1}p_A^{2Z_1}(p_Ap_O)^{y_2 - Z_1}p_B^{2Z_2}(p_Bp_O)^{y_3-Z_2}p_O^{2y_4} &\\\\\n",
        "    &= p_A^{m_1}p_B^{m_2}p_O^{m_3}\n",
        "\\end{align*}$\n",
        "\n",
        "where $m_1 = y_1 + y_2 + Z_1$, $m_2 = y_1 + y_3 + Z_2$ and $m_3 = y_2+y_3-Z_1-Z_2+2y_4$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLfootrA03WY"
      },
      "source": [
        "We then could use Gibbs Sampler to estimate the latent variables. The derived marginal distribution are given below, since we have no prior information we will set $a_1 = a_2 = a_3 = 1$\n",
        "$$\\mathbf{Z_1}|Y, p_A, p_B, p_O \\sim Binom(y_2, \\frac{p_A^2}{p_A^2 + 2p_A p_O})$$\n",
        "$$\\mathbf{Z_2}|Y, p_A, p_B, p_O \\sim Binom(y_3, \\frac{p_B^2}{p_B^2 + 2p_B p_O}) \\\\$$\n",
        "$$\\mathbf{\\theta} = p_A,p_B,p_O \\sim Dirichlet(m_1+a_1, m_2+a_2, m_3+a_3)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:28.819447Z",
          "start_time": "2021-01-27T21:43:28.815083Z"
        },
        "id": "USy5nZbZ03WY"
      },
      "source": [
        "def dirichlet(a, b, c):\n",
        "    x1 = np.random.gamma(a, 1)\n",
        "    x2 = np.random.gamma(b, 1)\n",
        "    x3 = np.random.gamma(c, 1)\n",
        "    s = x1 + x2 + x3\n",
        "    return np.array([x1,x2,x3])/s\n",
        "\n",
        "def RaoBlackwell(X1, X2, Z):\n",
        "    result = (X1 + X2 + Z + 1) / (2*y1 + 2*y2 + 2*y3 + 2*y4 + 3)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:28.827005Z",
          "start_time": "2021-01-27T21:43:28.822153Z"
        },
        "id": "162PXL7t03WY"
      },
      "source": [
        "#Sample Amount\n",
        "Sam = 3000\n",
        "\n",
        "Z1 = np.array([0.0]*Sam)\n",
        "Z2 = np.array([0.0]*Sam)\n",
        "theta = np.array([1.0]*Sam*3).reshape((Sam,3))\n",
        "y1 = 89; y2 = 642; y3 = 195; y4 = 657\n",
        "Z1[0] = 500; Z2[0] = 500\n",
        "\n",
        "theta[0] = [1/3, 1/3, 1/3]\n",
        "a1 = 1; a2 = 1; a3 = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:28.957158Z",
          "start_time": "2021-01-27T21:43:28.829489Z"
        },
        "id": "NjaVHxhZ03WZ"
      },
      "source": [
        "for i in range(1,Sam):\n",
        "    Z1[i] = np.random.binomial(y2,\n",
        "                               (theta[i-1, 0]**2)/((theta[i-1, 0]**2) + 2*theta[i-1, 0]*theta[i-1, 2]), 1)\n",
        "    Z2[i] = np.random.binomial(y3,\n",
        "                               (theta[i-1, 1]**2)/((theta[i-1, 1]**2) + 2*theta[i-1, 1]*theta[i-1, 2]), 1)\n",
        "\n",
        "    m1 = y1 + y2 + Z1[i]\n",
        "    m2 = y1 + y3 + Z2[i]\n",
        "    m3 = y2 + y3 - Z1[i] - Z2[i] + 2*y4\n",
        "\n",
        "    theta[i, ] = dirichlet(m1+a1, m2+a2, m3+a3)\n",
        "\n",
        "#Burn out the first 1000 samples\n",
        "theta = theta[1000:,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJr9dGt03WZ"
      },
      "source": [
        "## Rao Blackwellized Gibbs Sampler\n",
        "\n",
        "We then want to derive the Rao Blackwellized Gibbs Sampler using the following Bayesian formula. We assumed the sufficient statistics to be $\\mathbf{Y}, \\mathbf{Z}$\n",
        "\n",
        "Then\n",
        "$$P(p_A|p_B, p_O, \\mathbf{Y}, \\mathbf{Z}) = \\frac{P(p_A, p_B, p_O | \\mathbf{Y}, \\mathbf{Z}) \\times P(\\mathbf{Y}, \\mathbf{Z})}{P(p_B, p_O,\\mathbf{Y}, \\mathbf{Z})} =\\frac{P(p_A, p_B, p_O|\\mathbf{Y}, \\mathbf{Z})}{P(p_B, p_O|\\mathbf{Y}, \\mathbf{Z})}$$\n",
        "\n",
        "\n",
        "$$p_A|p_B, p_O, \\mathbf{Y}, \\mathbf{Z} \\sim Beta(m_1+a_1, m_2+a_2 + m_3+a_3)$$\n",
        "\n",
        "We could follow the same idea to derive the Monte Carlo estimation of $p_A$, $p_B$, and $p_O$.\n",
        "$$\\hat{p_A} = \\frac{m_1 + 1}{m_1 + m_2 + m_3 + 3} = \\frac{1}{M} \\sum_i^M \\frac{y_1 + y_2 + Z_{1i}+1}{2(y_1 + y_2 + y_3 + y_4)+3}$$\n",
        "$$\\hat{p_B} = \\frac{m_2 + 1}{m_1 + m_2 + m_3 + 3} = \\frac{1}{M} \\sum_i^M \\frac{y_1 + y_3 + Z_{2i}+1}{2(y_1 + y_2 + y_3 + y_4)+3}$$\n",
        "$$\\hat{p_O} = \\frac{m_3 + 1}{m_1 + m_2 + m_3 + 3} = \\frac{1}{M} \\sum_i^M \\frac{y_2 + y_3 - Z_{1i} - Z_{2i}+2y_4}{2(y_1 + y_2 + y_3 + y_4)+3}$$\n",
        "\n",
        "The following plots showed the Rao BlackWellized Estimators have lower variance than the Gibbs Sampler Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.182772Z",
          "start_time": "2021-01-27T21:43:28.958633Z"
        },
        "id": "nPvV8mUK03WZ"
      },
      "source": [
        "fib, ax = plt.subplots()\n",
        "sns.set_style('whitegrid')\n",
        "sns.kdeplot(theta[:,0], bw=0.001, ax = ax, label=\"Gibbs\")\n",
        "sns.kdeplot(RaoBlackwell(y1, y2, Z1), bw=0.001, ax = ax, label=\"Rao Blackwellized Gibbs\")\n",
        "ax.set_title('Estiamtion of P(A)')\n",
        "ax.set(xlabel=\"Probability\", ylabel = \"Frequency\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.540284Z",
          "start_time": "2021-01-27T21:43:29.184532Z"
        },
        "scrolled": false,
        "id": "WmI_Nysd03Wa"
      },
      "source": [
        "fib, ax = plt.subplots()\n",
        "sns.set_style('whitegrid')\n",
        "sns.kdeplot(theta[:,1], bw=0.001, ax = ax, label=\"Gibbs\")\n",
        "sns.kdeplot(RaoBlackwell(y1, y3, Z2), bw=0.001, ax = ax, label=\"Rao Blackwellized Gibbs\")\n",
        "ax.set_title('Estiamtion of P(B)')\n",
        "ax.set(xlabel=\"Probability\", ylabel = \"Frequency\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.849558Z",
          "start_time": "2021-01-27T21:43:29.542135Z"
        },
        "id": "3QHpswoU03Wa"
      },
      "source": [
        "fib, ax = plt.subplots()\n",
        "sns.set_style('whitegrid')\n",
        "sns.kdeplot(theta[:,2], bw=0.001, ax = ax, label=\"Gibbs\")\n",
        "sns.kdeplot(RaoBlackwell(y2+y3, 2*y4, -Z2-Z1), bw=0.001, ax = ax, label=\"Rao Blackwellized Gibbs\")\n",
        "ax.set_title('Estiamtion of P(O)')\n",
        "ax.set(xlabel=\"Probability\", ylabel = \"Frequency\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgd1udmG03Wb"
      },
      "source": [
        "# Re-implementation\n",
        "\n",
        "## The Unordered Set Policy Gradient Estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN2mfLG603Wb"
      },
      "source": [
        "Our goal is to estimate the expectation of $f(\\mathbf{x})$ where $\\mathbf{x}$ has a discrete distribution $p$ over the domain $D$, i.e.\n",
        "\n",
        "$$\\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})}[f(\\mathbf{x})] = \\sum_{\\mathbf{x} \\in D}p(\\mathbf{x})f(\\mathbf{x})$$\n",
        "\n",
        "\n",
        "Then let $S^k\\subseteq D$ be an unordered sample without replacement from the distribtuion $p$, $s\\in S^k$ be the elements in the sample, $p_{\\boldsymbol{\\theta}}$ indicates the dependency on the model parameters $\\boldsymbol{\\theta}$, define the $\\textit{unordered set policy gradient estimator}$ as \n",
        "\n",
        "$$e^{U S P G}\\left(S^{k}\\right)=\\sum_{s \\in S^{k}} p_{\\boldsymbol{\\theta}}(s) R\\left(S^{k}, s\\right) \\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(s) f(s)=\\sum_{s \\in S^{k}} \\nabla_{\\boldsymbol{\\theta}} p_{\\boldsymbol{\\theta}}(s) R\\left(S^{k}, s\\right) f(s)$$\n",
        "\n",
        "where $\\displaystyle R\\left(S^{k}, s\\right)=\\frac{p^{D \\backslash\\{s\\}}\\left(S^{k} \\backslash\\{s\\}\\right)}{p\\left(S^{k}\\right)}$ is the leave-one-out ratio.\n",
        "\n",
        "To reduce the variance, we can construct a baseline and substract it from $f$. The $\\textit{unordered set policy gradient estimator with baseline}$ is given by\n",
        "\n",
        "$$e^{U S P G B L}\\left(S^{k}\\right)=\\sum_{s \\in S^{k}} \\nabla_{\\boldsymbol{\\theta}} p_{\\boldsymbol{\\theta}}(s) R\\left(S^{k}, s\\right)\\left(f(s)-\\sum_{s^{\\prime} \\in S^{k}} p_{\\boldsymbol{\\theta}}\\left(s^{\\prime}\\right) R^{D \\backslash\\{s\\}}\\left(S^{k}, s^{\\prime}\\right) f\\left(s^{\\prime}\\right)\\right)$$\n",
        "\n",
        "where $\\displaystyle R^{D \\backslash\\{s\\}}\\left(S^{k}, s^{\\prime}\\right)=\\frac{p_{\\boldsymbol{\\theta}}^{D \\backslash\\left\\{s, s^{\\prime}\\right\\}}\\left(S^{k} \\backslash\\left\\{s, s^{\\prime}\\right\\}\\right)}{p_{\\boldsymbol{\\theta}}^{D \\backslash\\{s\\}}\\left(S^{k} \\backslash\\{s\\}\\right)}$ is the second order leave-one-out ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGqtOBoP03Wb"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikLHOBIE03Wc"
      },
      "source": [
        "The experiment modifies the Bernoulli toy experiment in the paper \"Estimating Gradients for Discrete Random Variables by Sampling without Replacement\" and the code is adapted from https://github.com/wouterkool/estimating-gradients-without-replacement/tree/master/bernoulli from the GitHub of the paper. In this experiment, given a vector $\\mathbf{p}=(0.6,0.51,0.48)$, the goal is to minimize\n",
        "\n",
        "$$\\mathbb{E}_{x_1,x_2,x_3\\sim p(\\sigma(\\eta))}[\\sum_{i=1}^3(x_i-p_i)^2]$$\n",
        "where $x_1,x_2,x_3$ are i.i.d. from Geometric($\\sigma(\\eta)$)/Binominal(4,$\\sigma(\\eta)$) distribution and $\\eta\\in\\mathbb{R}$, $\\sigma(\\eta)=(1+\\exp(-\\eta))^{-1}$ is the sigmoid function. We compare the variance of the unordered set policy gradient estimator with other estimators, with and without baseline, and set $\\eta=0.0,4.0$.\n",
        "\n",
        "We compare it with stratified/systematic sampling, VIMCO and other estimators, and present the plots of their gradient variance(log scale) for two distributions of different $\\eta$ below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.855251Z",
          "start_time": "2021-01-27T21:43:29.852641Z"
        },
        "id": "rf7uIo_Y03Wc"
      },
      "source": [
        "softmax = nn.Softmax(dim = 0)\n",
        "sigmoid = nn.Sigmoid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.863581Z",
          "start_time": "2021-01-27T21:43:29.858397Z"
        },
        "id": "kCSok_j203Wc"
      },
      "source": [
        "np.random.seed(454)\n",
        "_ = torch.manual_seed(454)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.875757Z",
          "start_time": "2021-01-27T21:43:29.865008Z"
        },
        "id": "ykxYHUqn03Wd"
      },
      "source": [
        "# fixed parameters\n",
        "d = 3\n",
        "# p0 = torch.rand(d)\n",
        "p0 = torch.Tensor([0.6, 0.51, 0.48])\n",
        "print('p0: ', p0, '\\n')\n",
        "\n",
        "print('sum(p0^2): ', torch.sum(p0**2))\n",
        "print('sum((1 - p0)^2): ', torch.sum((1 - p0)**2), '\\n')\n",
        "\n",
        "# the optima\n",
        "x_optimal = torch.argmin(torch.Tensor([torch.sum(p0**2), torch.sum((1 - p0)**2)]))\n",
        "\n",
        "optimal_loss = torch.min(torch.Tensor([torch.sum(p0**2), torch.sum((1 - p0)**2)]))\n",
        "\n",
        "print('optimal loss: ', optimal_loss)\n",
        "print('optimal x: ', x_optimal.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.883210Z",
          "start_time": "2021-01-27T21:43:29.877452Z"
        },
        "id": "fGIrl25c03Wd"
      },
      "source": [
        "# random init for phi\n",
        "phi0 = torch.Tensor([0.0])\n",
        "phi0.requires_grad_(True)\n",
        "print('init phi0: ', phi0)\n",
        "print('init e_b: ', sigmoid(phi0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.889301Z",
          "start_time": "2021-01-27T21:43:29.885684Z"
        },
        "scrolled": true,
        "id": "rDrWiUck03Wd"
      },
      "source": [
        "print(phi0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.962892Z",
          "start_time": "2021-01-27T21:43:29.891388Z"
        },
        "id": "NLjhYWAs03We"
      },
      "source": [
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "n_samples = 1000\n",
        "\n",
        "def get_gradient_estimators(phi0,dist):\n",
        "    phi0.requires_grad_(True)\n",
        "    params = [phi0]\n",
        "    experiment = ab_lib.BernoulliExperiments(p0, d, phi0)\n",
        "    experiment.set_var_params(deepcopy(phi0))\n",
        "    optbl = experiment.get_full_loss(dist).item()\n",
        "\n",
        "    def get_grad_unordered(k, baseline_constant=None):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = 0, \n",
        "            grad_estimator = bs_lib.reinforce_unordered,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs = {\n",
        "                'n_samples': k,\n",
        "                'baseline_constant': baseline_constant\n",
        "            },dist=dist)\n",
        "\n",
        "    def get_grad_unordered_optbl(k):\n",
        "        return get_grad_unordered(k, baseline_constant=optbl)\n",
        "\n",
        "    def get_grad_unordered_nobl(k):\n",
        "        return get_grad_unordered(k, baseline_constant=0.0)\n",
        "    \n",
        "    def get_grad_stratified(k, systematic=False):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = 0, \n",
        "            grad_estimator = bs_lib.stratified,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs = {\n",
        "                'n_samples': k,\n",
        "                'systematic': systematic\n",
        "            },dist=dist)\n",
        "    \n",
        "    def get_grad_systematic(k):\n",
        "        return get_grad_stratified(True)\n",
        "    \n",
        "    def get_grad_reinforce_wr(k, baseline_constant=None):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = 0, \n",
        "            grad_estimator = bs_lib.reinforce_wr,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs = {\n",
        "                'n_samples': k,\n",
        "                'baseline_constant': baseline_constant\n",
        "            },dist=dist)\n",
        "    \n",
        "    def get_grad_reinforce_wr_optbl(k):\n",
        "        return get_grad_reinforce_wr(k, baseline_constant=optbl)\n",
        "    \n",
        "    def get_grad_reinforce_wr_nobl(k):\n",
        "        return get_grad_reinforce_wr(k, baseline_constant=0.0)\n",
        "\n",
        "    def get_grad_topk(k, sample_topk=False):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = k - 1, \n",
        "            sample_topk = sample_topk,\n",
        "            grad_estimator = bs_lib.reinforce,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs={'grad_estimator_kwargs': None},dist=dist)\n",
        "\n",
        "    def get_grad_topk_bl(k, sample_topk=False):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = k - 1, \n",
        "            sample_topk = sample_topk,\n",
        "            grad_estimator = bs_lib.reinforce_w_double_sample_baseline,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs={'grad_estimator_kwargs': None},dist=dist)\n",
        "\n",
        "    def get_grad_topk_optbl(k, sample_topk=False):\n",
        "        return ab_lib.sample_gradient(\n",
        "            phi0, experiment, \n",
        "            topk = k - 1, \n",
        "            grad_estimator = bs_lib.reinforce,\n",
        "            n_samples = n_samples,\n",
        "            grad_estimator_kwargs={'grad_estimator_kwargs': None, 'baseline': optbl},\n",
        "            dist=dist)\n",
        "\n",
        "    def get_grad_topk_sample(k):\n",
        "        return get_grad_topk(k, sample_topk=True)\n",
        "\n",
        "    def get_grad_topk_bl_sample(k):\n",
        "        return get_grad_topk_bl(k, sample_topk=True)\n",
        "\n",
        "    def get_grad_topk_optbl_sample(k):\n",
        "        return get_grad_topk_optbl(k, sample_topk=True)\n",
        "\n",
        "    alpha = 0.2\n",
        "\n",
        "    xrng = np.arange(2 ** d) + 1\n",
        "    n_eval = xrng\n",
        "    n_eval_double = 2 * xrng\n",
        "    n_eval_optbl = xrng + 2 ** d # Need all evaluations for baseline\n",
        "    \n",
        "    c_vimco = colors[1]\n",
        "    c_detsas = colors[2]\n",
        "    c_stochsas = colors[3]\n",
        "    c_unord = colors[0]\n",
        "    c_stratified = colors[4]\n",
        "    c_systematic = colors[5]\n",
        "    \n",
        "    estimators = [\n",
        "        \n",
        "        (get_grad_stratified, \"Stratified (no bl)\", n_eval, {\n",
        "            'color': c_stratified, 'linestyle': '--', 'marker': '.'}),\n",
        "        (get_grad_systematic, \"Systematic (no bl)\", n_eval, {\n",
        "            'color': c_systematic, 'linestyle': '--', 'marker': '.'}),\n",
        "        (get_grad_reinforce_wr_nobl, \"VIMCO (no bl)\", n_eval, {\n",
        "            'color': c_vimco, 'linestyle': '--', 'marker': '.'}),\n",
        "        (get_grad_topk, \"Det. sum & sample (no bl)\", n_eval, {\n",
        "            'color': c_detsas, 'linestyle': '--', 'marker': '.'}),\n",
        "        (get_grad_topk_sample, \"Stoch. sum & sample (no bl)\", n_eval, {\n",
        "            'color': c_stochsas, 'linestyle': '--', 'marker': '.'}),\n",
        "        (get_grad_unordered_nobl, \"Unordered (no bl)\", n_eval, {\n",
        "            'color': c_unord, 'linestyle': '--', 'marker': '.'}),\n",
        "        \n",
        "        (get_grad_reinforce_wr, \"VIMCO (built-in bl)\", n_eval, {\n",
        "            'color': c_vimco, 'linestyle': '-', 'marker': '.'}),\n",
        "        (get_grad_topk_bl, \"Det. sum & sample (sample bl)\", n_eval_double, {\n",
        "            'color': c_detsas, 'linestyle': '-', 'marker': '.'}),\n",
        "        (get_grad_topk_bl_sample, \"Stoch. sum & sample (sample bl)\", n_eval_double, {\n",
        "            'color': c_stochsas, 'linestyle': '-', 'marker': '.'}),\n",
        "        (get_grad_unordered, \"Unordered (built-in bl)\", n_eval, {\n",
        "            'color': c_unord, 'linestyle': '-', 'marker': '.'}),\n",
        "    ]\n",
        "    return estimators"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.971967Z",
          "start_time": "2021-01-27T21:43:29.965602Z"
        },
        "scrolled": true,
        "id": "YQv2tk4K03We"
      },
      "source": [
        "def run_experiments(estimators, n_samples):\n",
        "    mixed_grad_array_all = torch.zeros(len(estimators), (2**d), n_samples)\n",
        "\n",
        "    for i in range(2**d):\n",
        "        k = i + 1\n",
        "        print(f\"k = {k}\")\n",
        "\n",
        "        for j, (estimator, name, n_ev, plot_kwargs) in enumerate(estimators):\n",
        "            print(name)\n",
        "            try:\n",
        "                grads = estimator(k)\n",
        "                mixed_grad_array_all[j, i] = grads\n",
        "            except:\n",
        "                raise\n",
        "                print(f\"Warn: {name} with {k} evaluations failed\")\n",
        "    return mixed_grad_array_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:43:29.984801Z",
          "start_time": "2021-01-27T21:43:29.975311Z"
        },
        "scrolled": false,
        "id": "fchvjiBc03Wf"
      },
      "source": [
        "def plot_results(estimators, mixed_grad_array_all, phi0, ylim=None, inc_baseline=False):\n",
        "\n",
        "    mixed_grad_means = mixed_grad_array_all.mean(-1).numpy()\n",
        "    mixed_grad_stds = mixed_grad_array_all.std(-1).numpy()\n",
        "    \n",
        "    mixed_grad_stds[mixed_grad_stds < 1e-8]  = 0\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    xrng = np.arange(mixed_grad_stds.shape[1]) + 1\n",
        "    estimator_names = []\n",
        "    for grad_arr, (est, name, n_ev, plot_kwargs) in zip(mixed_grad_stds, estimators):\n",
        "        plt.plot(n_ev if inc_baseline else xrng, grad_arr ** 2, **plot_kwargs, lw='2')\n",
        "        estimator_names.append(name)\n",
        "\n",
        "    incl_text = \"incl\" if inc_baseline else \"excl\"\n",
        "    plt.xlabel(f'Number of evaluations ({incl_text}. baseline)')\n",
        "    plt.ylabel('Gradient variance (log scale)')\n",
        "    if ylim is not None:\n",
        "        plt.ylim(ylim)\n",
        "    eta = phi0.detach().numpy()[0]\n",
        "    plt.title('$\\eta$ = {}'.format(eta))\n",
        "    if ylim is None:\n",
        "        plt.legend(estimator_names)\n",
        "    plt.yscale('log')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    #fig.savefig(\"plots/toy_eta{}_{}.pdf\".format(int(eta), \"inc\" if inc_baseline else \"exc\"),\n",
        "    #            format='pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:48:06.656792Z",
          "start_time": "2021-01-27T21:43:29.987620Z"
        },
        "id": "4qQ0nxkw03Wf"
      },
      "source": [
        "# run for eta=0.0 and 4.0 for Binominal Distribution\n",
        "phi0 = torch.Tensor([0.0])\n",
        "phi0_hard = torch.Tensor([-4.0])\n",
        "estimators = get_gradient_estimators(phi0,dist='bin')\n",
        "estimators_hard = get_gradient_estimators(phi0_hard,dist='bin')\n",
        "mixed_grad_array_all = run_experiments(estimators, n_samples)\n",
        "mixed_grad_array_all_hard = run_experiments(estimators_hard, n_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:48:06.671164Z",
          "start_time": "2021-01-27T21:43:27.982Z"
        },
        "id": "BfDENAzV03Wf"
      },
      "source": [
        "# plot the figure\n",
        "inc_baseline=True\n",
        "plot_results(estimators, mixed_grad_array_all, phi0, ylim=None, \n",
        "             inc_baseline=inc_baseline)\n",
        "plot_results(estimators_hard, mixed_grad_array_all_hard, phi0_hard, ylim=None, \n",
        "             inc_baseline=inc_baseline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:48:06.677085Z",
          "start_time": "2021-01-27T21:43:27.984Z"
        },
        "id": "KjWUdfgt03Wg"
      },
      "source": [
        "# run for eta=0.0 and 4.0 for Geometric Distribution\n",
        "phi0 = torch.Tensor([0.0])\n",
        "phi0_hard = torch.Tensor([-4.0])\n",
        "estimators = get_gradient_estimators(phi0,dist='geo')\n",
        "estimators_hard = get_gradient_estimators(phi0_hard,dist='geo')\n",
        "mixed_grad_array_all = run_experiments(estimators, n_samples)\n",
        "mixed_grad_array_all_hard = run_experiments(estimators_hard, n_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-27T21:48:06.678788Z",
          "start_time": "2021-01-27T21:43:27.985Z"
        },
        "id": "z3kfiXfi03Wg"
      },
      "source": [
        "inc_baseline=True\n",
        "plot_results(estimators, mixed_grad_array_all, phi0, ylim=None, \n",
        "             inc_baseline=inc_baseline)\n",
        "plot_results(estimators_hard, mixed_grad_array_all_hard, phi0_hard, ylim=None, \n",
        "             inc_baseline=inc_baseline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm67o8RI03Wg"
      },
      "source": [
        "We can see from the above two graphs, as the number of evaluations increases, our estimator with baseline does have the lowest gradient variance among all the estimators, except for Binominal distribution at $\\eta=4$ as the evaluation number is less than $8$. Use the experiments, we prove that the power of reducing the variance, however sometimes it depends on the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5dUPb7V03Wg"
      },
      "source": [
        "# Reference\n",
        "1. Gibbs Sampler and Rao Blackwellization, math et al 2020-06-07: https://rpubs.com/mathetal/raoblackwellization\n",
        "1. The Calculation of Posterior Distributions by Data Augmentation, Matin A. Tanner and Wing. H. Wong 2009-09-03\n",
        "1. Bernoulli gradient variance, Wouter Kool, Herke van Hoof, Max Welling https://github.com/wouterkool/estimating-gradients-without-replacement/tree/master/bernoulli"
      ]
    }
  ]
}